import pickle
from collections import defaultdict, Counter
from tqdm import tqdm
import pandas as pd
import glob
import pyarrow.parquet as pq
import pyarrow as pa
import os
import json

train_path = "dataset/train.jsonl"
co_vis_pkl = "dataset_1/co_visitation.pkl"
output_dir = "dataset_1"
os.makedirs(output_dir, exist_ok=True)

# ========== åŠ è½½å·²æœ‰çš„å…±ç°çŸ©é˜µ ==========
with open(co_vis_pkl, "rb") as f:
    co_vis = pickle.load(f)

co_visitation_clicks = co_vis["clicks"]
co_visitation_cart = co_vis["cart"]
co_visitation_order = co_vis["order"]
print(f"âœ… å…±ç°çŸ©é˜µå·²åŠ è½½ ({co_vis_pkl})")

# ========== å…¨å±€çƒ­é—¨ (å€™é€‰è¡¥ä½ç”¨) ==========
global_popular = Counter()
with open(train_path, "r", encoding="utf-8") as f:
    for line in tqdm(f, desc="ç»Ÿè®¡å…¨å±€çƒ­é—¨"):
        session = json.loads(line)
        for e in session['events']:
            global_popular[e['aid']] += 1

global_popular_top = [aid for aid, _ in global_popular.most_common(50)]

# ========== å€™é€‰é›†ç”Ÿæˆ ==========
weights = {"clicks": 1.0, "cart": 1.5, "order": 2.0}
batch_size = 100000
candidate_rows = []
part_id = 0

def count_lines(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        return sum(1 for _ in f)

total_lines = count_lines(train_path)
total_batches = (total_lines // batch_size) + 1

with open(train_path, 'r', encoding='utf-8') as f, tqdm(f, total=total_lines, desc='ç”Ÿæˆå€™é€‰é›†') as session_pbar:
    for i, line in enumerate(session_pbar):
        session = json.loads(line)
        sid = session['session']
        events = session['events']

        clicked = [e['aid'] for e in events if e['type'] == 'clicks']
        if not clicked:
            continue

        candidates = defaultdict(int)
        for idx, aid in enumerate(clicked[-20:]):  # æœ€è¿‘ 20 æ¬¡ç‚¹å‡»
            decay = (idx + 1) / 20  # æ—¶é—´è¡°å‡
            for b, score in co_visitation_clicks.get(aid, {}).items():
                candidates[b] += decay * weights["clicks"] * score
            for b, score in co_visitation_cart.get(aid, {}).items():
                candidates[b] += decay * weights["cart"] * score
            for b, score in co_visitation_order.get(aid, {}).items():
                candidates[b] += decay * weights["order"] * score

        # å– top50
        top_items = sorted(candidates.items(), key=lambda x: -x[1])[:50]
        for aid, score in top_items:
            candidate_rows.append({
                "session": sid,
                "candidate": aid,
                "score": score,
                "source": "co_vis"
            })

        # çƒ­é—¨è¡¥ä½
        for aid in global_popular_top:
            if aid not in candidates:
                candidate_rows.append({
                    "session": sid,
                    "candidate": aid,
                    "score": 0,
                    "source": "popular"
                })

        # æ‰¹é‡å†™å…¥
        if (i + 1) % batch_size == 0:
            df = pd.DataFrame(candidate_rows)
            df.to_parquet(f"{output_dir}/candidates_part_{i // batch_size}.parquet", index=False)
            candidate_rows = []
            part_id += 1

# æœ€åä¸€æ‰¹
if candidate_rows:
    df = pd.DataFrame(candidate_rows)
    df.to_parquet(f"{output_dir}/candidates_part_last.parquet", index=False)
    part_id += 1

print("âœ… å€™é€‰é›†å·²åˆ†æ‰¹ä¿å­˜")

# ========== åˆå¹¶æ–‡ä»¶ ==========
files = sorted(glob.glob(f"{output_dir}/candidates_part_*.parquet"))
writer = None

with tqdm(total=len(files), desc="åˆå¹¶å€™é€‰é›†æ–‡ä»¶") as merge_pbar:
    for f in files:
        df = pd.read_parquet(f)
        table = pa.Table.from_pandas(df)
        if writer is None:
            writer = pq.ParquetWriter(f"{output_dir}/candidates.parquet", table.schema)
        writer.write_table(table)
        merge_pbar.update(1)

if writer:
    writer.close()

print(f"ğŸ‰ å€™é€‰é›†å·²æˆåŠŸåˆå¹¶ä¿å­˜åˆ° {output_dir}/candidates.parquet")
